### tf.sigmoid

```
tf.math.sigmoid(
    x,
    name=None
)
```

### tf.tanh

```
tf.math.tanh(
    x,
    name=None
)
```

### tf.nn.relu

```
tf.nn.relu(
    features,
    name=None
)
```

### tf.nn.leaky_relu

```
tf.nn.leaky_relu(
    features,
    alpha=0.2,
    name=None
)
```

### tf.nn.softmax

```
tf.nn.softmax(
    logits,
    axis=None,
    name=None,
    dim=None
)
```

### tf.nn.sigmoid_cross_entropy_with_logits

```
tf.nn.sigmoid_cross_entropy_with_logits(
    _sentinel=None,
    labels=None,
    logits=None,
    name=None
)
```

```
#For brevity, let x = logits, z = labels. The logistic loss is
  z * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x))
= z * -log(1 / (1 + exp(-x))) + (1 - z) * -log(exp(-x) / (1 + exp(-x)))
= z * log(1 + exp(-x)) + (1 - z) * (-log(exp(-x)) + log(1 + exp(-x)))
= z * log(1 + exp(-x)) + (1 - z) * (x + log(1 + exp(-x))
= (1 - z) * x + log(1 + exp(-x))
= x - x * z + log(1 + exp(-x))

#For x < 0, to avoid overflow in exp(-x), we reformulate the above
  x - x * z + log(1 + exp(-x))
= log(exp(x)) - x * z + log(1 + exp(-x))
= - x * z + log(1 + exp(x))

#Hence, to ensure stability and avoid overflow, the implementation uses this equivalent formulation
max(x, 0) - x * z + log(1 + exp(-abs(x)))
```

### tf.nn.softmax_cross_entropy_with_logits_v2

```
tf.nn.softmax_cross_entropy_with_logits_v2(
    labels,
    logits,
    axis=None,
    name=None,
)
```

tf.nn.softmax_cross_entropy_with_logits 是即将失效的函数，和 V2 的区别在于，反向传播时，v2 可以同时作用于 labels 和 logits。若需要保持 labels 不变，需对 lables 参数设置 stop_gradients 函数。

### tf.stop_gradient

```
tf.stop_gradient(
    input,
    name=None
)
```

### tf.nn.sparse_softmax_cross_entropy_with_logits

```
tf.nn.sparse_softmax_cross_entropy_with_logits(
    _sentinel=None,
    labels=None,
    logits=None,
    name=None
)
```
